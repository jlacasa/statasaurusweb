[
  {
    "objectID": "code_3_bayes_2.html",
    "href": "code_3_bayes_2.html",
    "title": "Code #2",
    "section": "",
    "text": "This is a follow-up article from Bayes#1. Still, we do have numerous important concepts in order to understand what the computational codes are doing behind scenes when running a Bayesian analysis.\n\n\n\n\n\n\nImportant\n\n\n\nToday’s Topics:\nComputing posterior distributions:\n#1. Acceptance/Rejection Sampling Basics:\n#2. Markov Chain Monte Carlo (MCMC) More efficient than AR sampling.\nPackages for Bayesian analysis in R:\n#3. brms\n#4. rstan\n#5. rjags\n\n\n\n\n\n\n1. Generate proposal parameter values 2. Generate data with those parameters\n3. Compare the simulated data with the observed data = “difference” 4. “Accept” that combination of parameters if the difference < predifined acceptable error. “Reject” if the difference > predifined acceptable error.\nSee an example:\nUsing data of yield vesus plant density in corn:\n\n\n\n\n\n\\[ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2\\]\n\nGenerate proposal parameter values using the prior ditributions:\n\n\\[\\beta_0 \\sim uniform(4, 6)\\]\n\\[\\beta_1 \\sim uniform(1, 3)\\]\n\\[\\beta_2 \\sim uniform(0.5, 2)\\]\n\\[\\sigma \\sim Gamma(2, 2)\\]\n\nset.seed(567)\nb0_try <- runif(1, 4, 6)  # Parameter model\nb1_try <- runif(1, 1, 3)  # Parameter model \nb2_try <- rgamma(1, .5, 2) # Mathematical equation for process model\nmu_try <- b0_try + x*b1_try - (x^2)*b2_try\nsigma_try <- rgamma(1, 2, 2)\n\n\nGenerate data with those parameters\n\n\n\nset.seed(567)\ny_try <- rnorm(n, mu_try, sigma_try)  # Process model\n\n\nCompare the simulated data with the observed data = “difference”\n\n\n# Record difference between draw of y from prior predictive distribution and\n# observed data\ndiff[k, ] <- sum(abs(y - y_try))\n\n\n“Accept” (gold) that combination of parameters if the difference < predifined acceptable error. “Reject” (red) if the difference > predifined acceptable error.\n\n\nplot(x, y, xlab = \"Plant density\", \n     ylab = \"Observed yield\", xlim = c(2, 13), ylim = c(5, 20),\n     typ = \"b\", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))\npoints(x, y_hat[k,], typ = \"b\", lwd = 2, \n       col = ifelse(diff[1] < error, \"gold\", \"tomato\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, what if whe change the priors:\n\n\n\n\n\nNow, do many tries\n\nfor (k in 1:K_tries) {\n    \n    b0_try <- runif(1, 2, 10)  # Parameter model\n    b1_try <- rnorm(1, 2.2, .5)  # Parameter model \n    b2_try <- rgamma(1, .25, 2) # Mathematical equation for process model\n    mu_try <- b0_try + x*b1_try - (x^2)*b2_try\n    sigma_try <- rgamma(1, 2, 2)\n\n    y_try <- rnorm(n, mu_try, sigma_try)  # Process model\n    \n    # Record difference between draw of y from prior predictive distribution and\n    # observed data\n    diff[k, ] <- sum(abs(y - y_try))\n    \n    # Save unkown random variables and parameters\n    y_hat[k, ] <- y_try\n    \n    posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)\n}\n\nAcceptance rate\n\nlength(which(diff < error))/K_tries\n\n[1] 0.035985\n\n\nPriors versus posteriors:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhist(y_hat[which(diff < error), 25], col = \"grey\", freq = FALSE)\nabline(v = y[25], col = 'gold', lty = \"dashed\", lwd = 5)\n\n\n\n\n\n\n\n\n\nLet’s get started\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentation: https://paul-buerkner.github.io/brms/\nBug-reports: https://github.com/paul-buerkner/brms/issues\nbrms is a very handy R-package that facilitates running Bayesian models using a relatively simple syntax. It is basically and interface that runs “Stan” behind the scenes. It uses a syntax quite similar to the lme4 package.\nIt allows to use several different type of distributions and link functions for models that are linear, counts, survival, response, ordinal, zero-inflated, etc.\nDue to its relatively simple syntax, today, we are going to start our Bayesian coding with brms.\nMore about brms at https://www.jstatsoft.org/article/view/v080i01\n\n\n\n\n\nDocumentation: https://mc-stan.org/rstan/\nBug reports: https://github.com/stan-dev/rstan/issues/\nstan is a stand-alone open-source software platform designed for statistical modeling using high-performance statistical computation applying its own language. When selecting the Bayesian computational approach (i.e. rejection sampling criteria) there are several alternatives to choose. Stan produces Bayesian statistical inference following Hamiltonian Monte Carlo (HMC), and No-U-Turn Samples (NUTS). Besides R, stan has interfaces with other popular languages such as Python, MATLAB, Julia.\nIn contrast to brms, stan’s syntax is more complicated for begginers, but the positive side is that requires us to write the statistical model.\nWe will not fit a model directly with stan today, but brms brings a function that allows users to obtain the code to run the analysis by ourselves using rstan. Let’s see…\n\n\n\n\nDocumentation: https://mcmc-jags.sourceforge.io/\nBug reports: https://sourceforge.net/projects/mcmc-jags/\nrjags is another popular option for Bayesian statistical inference following MCMC using R. Rjags produces Bayesian statistical inference following BUGS language (WinBUGS). Similar to stan, rjags it is probably not for beginner, since it requires us to write out the statistical model (although it is always ideal). To extract the posteriors, it also requires coda, which is especially designed for summarizing and plotting MCMC simulations."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section is intended to serve as a repo where we can share docs, papers, links, tutorials, etc. useful for each topic as well as random content."
  }
]